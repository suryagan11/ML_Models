{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Performance Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "HtZ2yI1Lgh2I",
        "SqjqAVSQgh2S",
        "Yh4fSgV9gh2a",
        "WlQf5BUtgh2o",
        "plCwrQB_gh23",
        "1_2Y9vQ9gh2_",
        "zY7eJaxsgh3K",
        "hum7bVaAgh3P",
        "GP8EUDXTgh3s",
        "AjIBhaItgh4X",
        "RHrz_JZAgh4j",
        "mx7Dtz5Wgh4r",
        "Onetrj_Agh5A",
        "gESg0k8_gh5H",
        "YoVnDQsigh5x",
        "QQwxdMUkgh6m",
        "1gp-Po0wgh7w"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdPA03q3vRei",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1JuiXnrL99eZzIiRzdtREyLlCYhF6Nc4u?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4yt1psqgh1X",
        "colab_type": "text"
      },
      "source": [
        "# Performance Evaluation, Cross Validation and Hyper - Parameter Tunning\n",
        "In this Notebook, we will learn 3 things:   \n",
        "*  Evaluation metrics\n",
        "*  Cross Validation\n",
        "*  Hyperparameter Tuning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57GJ0KnnpC34",
        "colab_type": "text"
      },
      "source": [
        "### Data Description\n",
        "Here we will use diabetes dataset for classification. Given different medical specifications about a person, we have to predict if the person have diabetes or not.\n",
        "\n",
        "**Different Attributes:**\n",
        "1. Number of times pregnant\n",
        "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
        "3. Diastolic blood pressure (mm Hg)\n",
        "4. Triceps skin fold thickness (mm)\n",
        "5. 2-Hour serum insulin (mu U/ml)\n",
        "6. Body mass index (weight in kg/(height in m)^2)\n",
        "7. Diabetes pedigree function\n",
        "8. Age (years)\n",
        "9. Class variable (0 or 1)\n",
        "\n",
        "All the feature names are numerical. Let's give textual names to these features.\n",
        "*  Number of times pregnant: **num_preg**\n",
        "*  Plasma glucose concentration a 2 hours in an oral glucose tolerance test: **plasma_glucose_conc**\n",
        "*  Diastolic blood pressure (mm Hg): **D_blood_pressure**\n",
        "*  Triceps skin fold thickness (mm): **skin_fold_thickness**\n",
        "*  2-Hour serum insulin (mu U/ml): **serum_insulin**\n",
        "*  Body mass index (weight in kg/(height in m)^2): **body_mass_index**\n",
        "*  Diabetes pedigree function: **pedigree_func**\n",
        "*  Age (years): **age**\n",
        "*  Class variable (0 or 1): **diabetes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYnubTr4oUKY",
        "colab_type": "text"
      },
      "source": [
        "### Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WQljMFOgh1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRSnYQG9okA4",
        "colab_type": "text"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUaCOsuroooy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# since the column names are numerical, we will give our own column names for our understanding\n",
        "col = [\"num_preg\", \"plasma_glucose_conc\", \"D_blood_pressure\", \"skin_fold_thickness\", \"serum_insulin\", \"body_mass_index\", \"pedigree_func\", \"age\", \"diabetes\"]\n",
        "diabetes_data = pd.read_csv(\"https://raw.githubusercontent.com/dphi-official/ML_Models/master/Performance_Evaluation/diabetes.txt\", names = col)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3Gg7tKkv8uw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c95cd712-111a-442b-d1e8-04a4b6f6cd22"
      },
      "source": [
        "diabetes_data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_preg</th>\n",
              "      <th>plasma_glucose_conc</th>\n",
              "      <th>D_blood_pressure</th>\n",
              "      <th>skin_fold_thickness</th>\n",
              "      <th>serum_insulin</th>\n",
              "      <th>body_mass_index</th>\n",
              "      <th>pedigree_func</th>\n",
              "      <th>age</th>\n",
              "      <th>diabetes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   num_preg  plasma_glucose_conc  ...  age  diabetes\n",
              "0         6                  148  ...   50         1\n",
              "1         1                   85  ...   31         0\n",
              "2         8                  183  ...   32         1\n",
              "3         1                   89  ...   21         0\n",
              "4         0                  137  ...   33         1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbHhT_yMgh1p",
        "colab_type": "text"
      },
      "source": [
        "**This dataset contains 13 columns and based on different features, it is guessed whether or not a person has Diabetes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67Eap_ejyyJ5",
        "colab_type": "text"
      },
      "source": [
        "### Separating Input variables and output variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly3PQ9DayxWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = diabetes_data.drop('diabetes', axis = 1)\n",
        "y = diabetes_data.diabetes"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOQVck62gh1z",
        "colab_type": "text"
      },
      "source": [
        "#### Split into training and testing (80:20)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AylKRHMgh12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
        "\n",
        "# The below line of code will not need to separate input variables and output variables.\n",
        "# The code is very simple if you remember numpy and pandas session. Indexing dataframe and arrays\n",
        "# x_train, x_test, y_train, y_test = train_test_split(diabetes.iloc[:, :-1], diabetes.iloc[:,-1], test_size=0.2, random_state=3)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3R17-E7kqjD",
        "colab_type": "text"
      },
      "source": [
        "**Note for learners:** Here we have used MLPClassifier from neural_network module of sklearn library. MLP Classifier is also a classification algorithm like logistic regression or decision tree. We will soon learn about Neural Networks and Artificial Neural Networks in the upcoming sessions. So, no need to worry about it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZizeBIHwgh17",
        "colab_type": "text"
      },
      "source": [
        "### Developing a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqeygkPIgh18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(max_iter=1000)\n",
        "mlp.fit(x_train, y_train)\n",
        "y_pred = mlp.predict(x_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8-k13W0gh1T",
        "colab_type": "text"
      },
      "source": [
        "## Performance Evaluation\n",
        "Evaluating performance of the machine learning model that we have built is an essential part of any machine learning project. Performance of our model is done using some evaluation metrics. Accuracy score is one among them.\n",
        "\n",
        "**Why accuracy score is not a good evaluation metric?**\n",
        "\n",
        "Our model may give satisfying results if we use accuracy score for a particular dataset but at the same time accuracy score is not a good measure of evaluation for some particular dataset like fraud detection that we discussed during class imbalance problem. Let's consider the same dataset, suppose we have 1000 transaction in the dataset. Out of 1000 transactions 20 transactions are fraud transactions. Now let's say you build a model which predicted all the 1000 transactions as not fraud transaction, for 980 transaction which were not fraud, the prediction is correct while the transaction which were actually fraud are also predicted as not fraud. The accuracy is nothing but total correct prediction divided by total prediction. In this case we have total prediction as 1000 (as we have 1000 transactions) while total correct prediction is 980, resulting the accuracy score of 980/1000 = 0.98. The model is giving 98% of accuracy. Do you think the model is good? No, because our model could not notice the transaction which were actually fraud. \n",
        "\n",
        "Here we will discuss some other metrices for both classification and regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D9DMXXigh1Z",
        "colab_type": "text"
      },
      "source": [
        "## 1. Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD9O15axgh2G",
        "colab_type": "text"
      },
      "source": [
        "**All performance metrics in sklearn are to be written in the same way -**  \n",
        "> ``` metric_function(true_label, predicted_labels) ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V1Az3YsBUgP",
        "colab_type": "text"
      },
      "source": [
        "Below are the metrics for classification problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtZ2yI1Lgh2I",
        "colab_type": "text"
      },
      "source": [
        "### Confusion Matrix\n",
        "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing.\n",
        "\n",
        "Further reading about confusion matrix and its related terminologies: \n",
        "1. https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/\n",
        "2. https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKr_nFuRgh2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3S3OpD6QLmq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d9129d8e-8ae6-415c-e3fe-d341f4c9fb08"
      },
      "source": [
        "confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[75, 17],\n",
              "       [31, 31]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12hGE_5rgh2O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ff588dd7-320b-4c69-84d5-ed337860af71"
      },
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()     # ravel() is used to convert a 2D array to 1D array. The output by confusion matrix is a 2D array.\n",
        "print(\"True Positive\", tp)\n",
        "print(\"True Negative\", tn)\n",
        "print(\"False Positive\", fp)\n",
        "print(\"False Negative\", fn)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True Positive 31\n",
            "True Negative 75\n",
            "False Positive 17\n",
            "False Negative 31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqjqAVSQgh2S",
        "colab_type": "text"
      },
      "source": [
        "### Accuracy\n",
        "\\begin{align}\n",
        "Accuracy = \\frac{TP+TN}{TP+TN+FN+FP}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AX3DELNUgh2U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80617d3c-c151-441a-cb72-c5fff241b2e9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "acc"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6883116883116883"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNFBGQaCEjCA",
        "colab_type": "text"
      },
      "source": [
        "**When is it good to use accuracy score as a model evaluation metric?**\n",
        "1. The classifications in the dataset is nearly symmetrical (means equal distribution of all the classes).\n",
        "2. The false positive and false negative on test data are nearly equal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh4fSgV9gh2a",
        "colab_type": "text"
      },
      "source": [
        "### Recall (Sensitivity)\n",
        "\\begin{align}\n",
        "Sensitivity = \\frac{TP}{TP+FN}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGBdDw9hgh2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import recall_score"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk9ffog9gh2g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0309eeed-ca93-4b2d-85d9-f7f3971ef5c6"
      },
      "source": [
        "recall_score(y_test, y_pred)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlQf5BUtgh2o",
        "colab_type": "text"
      },
      "source": [
        "### Specificity\n",
        "sklearn does not have an inbuild function for Specificity. But by adding parameter pos_label =0 to the recall function, we treat that as the positive class, and hence gives the correct output\n",
        "\\begin{align}\n",
        "Specificity = \\frac{TN}{TN+FP}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHTxK6NHgh2q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f2a0ff9-4dfa-4fe3-abc4-ff22c2e6ccb7"
      },
      "source": [
        "print(\"Specificity with recall pos label=0: \",recall_score(y_test, y_pred, pos_label=0))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Specificity with recall pos label=0:  0.8152173913043478\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UAM0Kz2gh2u",
        "colab_type": "text"
      },
      "source": [
        "**Checking with formulas (tn , fp from confusion matrix):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrthV4dhgh2v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd99bad4-b686-4756-d48c-d47a3a813317"
      },
      "source": [
        "print(\"Specificity with Formulas: \", tn/(tn+fp))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Specificity with Formulas:  0.8152173913043478\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7QrITQBgh21",
        "colab_type": "text"
      },
      "source": [
        "They are the same! You can use either one of them!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plCwrQB_gh23",
        "colab_type": "text"
      },
      "source": [
        "### Precision\n",
        "\\begin{align}\n",
        "Precision = \\frac{TP}{TP+FP}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14pynqe6gh25",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f3bc9bb-3b01-4013-f497-62c413a940df"
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "precision_score(y_test, y_pred)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6458333333333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_2Y9vQ9gh2_",
        "colab_type": "text"
      },
      "source": [
        "### Imbalanced Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwMwh4mhgh3A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f7e0da40-b05d-4108-e294-6c76789dd795"
      },
      "source": [
        "diabetes_data.iloc[:,-1].value_counts()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    500\n",
              "1    268\n",
              "Name: diabetes, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY7eJaxsgh3K",
        "colab_type": "text"
      },
      "source": [
        "### Matthews Correlation Coefficient\n",
        "\\begin{align}\n",
        "MCC = \\frac{(TP*TN)-(FP*FN)}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr62zEHWgh3L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5fe8aac5-3a86-4692-ccc0-be548b68ab15"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "print(\"MCC Score: \",matthews_corrcoef(y_test, y_pred))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC Score:  0.3337539218332509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hum7bVaAgh3P",
        "colab_type": "text"
      },
      "source": [
        "### F1 Score\n",
        "It is the harmonic mean of Precision and recall\n",
        "\n",
        "\\begin{align}\n",
        "Precision = \\frac{2*Precision*Recall}{Precision+Recall}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3A9uIKwgh3Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60635cd0-26ff-43c3-eb73-7e3521a1b854"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "print(\"F1 Score: \",f1_score(y_test, y_pred))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 Score:  0.5636363636363636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVcVsBEhgh3V",
        "colab_type": "text"
      },
      "source": [
        "## Area Under the Curve (Reciever Operating Characterstics)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YypfoTaVgh3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import plot_roc_curve"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZPwqn0Dgh3c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "736cafda-0895-4db3-df99-e378f1294d57"
      },
      "source": [
        "plot_roc_curve(mlp, x_test, y_test)\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de3xU9bnv8c9DBEEutgqcjWIMUqxcd5QAWkWhKFCkoMJWULdatd7b7qrUG4pYa+3G2mrr0VLrAXow3rpRWingtqF4tNwFBKyKiJqAisDhoiKXPvuPtZIOIcmskKxJZtb3/XrNi1lrfmvNs5Iwz/wu6/czd0dERJKrSUMHICIiDUuJQEQk4ZQIREQSTolARCThlAhERBLukIYOoLbatm3rBQUFDR2GiEhWWbp06afu3q6q17IuERQUFLBkyZKGDkNEJKuY2fvVvaamIRGRhFMiEBFJOCUCEZGEUyIQEUk4JQIRkYSLLRGY2RNm9omZrarmdTOzh81srZmtNLOT4opFRESqF2eNYAowtIbXvwV0CR9XAY/GGIuIiFQjtvsI3H2+mRXUUGQkMM2DebAXmNlXzKyDu2+MKyYRkcbmyYUf8MLyskhlux3Vhgnf7l7vMTRkH8HRwIcp26XhvgOY2VVmtsTMlmzatCkjwYmIZMILy8tYs3F7g8aQFXcWu/tkYDJAUVGRVtIRkZzSrUMbnr76lAZ7/4ZMBGXAMSnbHcN9IiJZoTbNOtVZs3E73Tq0qaeIDk5DNg3NBC4JRw+dDGxT/4CIZJP6aNbp1qENIwurbBXPmNhqBGZWDAwA2ppZKTABaArg7o8Bs4BhwFrgc+A7ccUiIhKXhm7WqQ9xjhoam+Z1B66P6/1FRCSarOgsFhFpLFL7BRpD+3590BQTIiK1kNov0Bja9+uDagQiIrWUC/0CqZQIRETSyMXmoFRqGhIRSSMXm4NSqUYgIhJBrjUHpVIiEBGppPIdw7nYHJRKTUMiIpVUvmM4F5uDUqlGICJShVxuCqpMiUBEck5dJ4PL9aagytQ0JCI5p66TweV6U1BlqhGISE5KUtNOXSkRiEjWS9oon/qmpiERyXpJG+VT31QjEJGcoKagg6cagYhIwikRiIgknBKBiEjCKRGIiCScOotFJDZ1vcM3Kg0XrRvVCEQkNnW9wzcqDRetG9UIRCRWGtbZ+KlGICKScEoEIiIJp0QgIpJw6iMQkSrVx4gfjebJDqoRiEiV6mPEj0bzZAfVCESkWhrxkwxKBCJSIbU5SM06yaGmIRGpkNocpGad5FCNQET2o+ag5Im1RmBmQ83sLTNba2a3VvF6vpmVmNnrZrbSzIbFGY+IiBwothqBmeUBjwBnAaXAYjOb6e5rUoqNB55x90fNrBswCyiIKyaRpIo6FFT9AskUZ42gL7DW3de5+27gKWBkpTIOlP/VHQ5siDEekcSKOhRU/QLJFGcfwdHAhynbpUC/SmXuBuaa2feAlsCZVZ3IzK4CrgLIz8+v90BFkkBt/1Kdhu4sHgtMcfefm9kpwO/NrIe7/yO1kLtPBiYDFBUVeQPEKZIVqmsCUpOP1CTOpqEy4JiU7Y7hvlRXAM8AuPvfgOZA2xhjEslp1TUBqclHahJnjWAx0MXMOhEkgDHAhZXKfAAMAqaYWVeCRLApxphEcp6agKS2YqsRuPte4AZgDvAmweig1WZ2j5mNCIvdBHzXzFYAxcBl7q6mHxGRDIq1j8DdZxEMCU3dd1fK8zXAqXHGICIiNdMUEyIiCdfQo4ZEcl59zOsflUYHycFQjUAkZvUxr39UGh0kB0M1ApEM0EgeacyUCERioHn9JZuoaUgkBprXX7KJagQiMVFzkGQL1QhERBJOiUBEJOGUCEREEi5yIjCzw+IMREREGkbazmIz+wbwONAKyDezfwWudvfr4g5OpK4yeVdvKg0ZlWwSpUbwC2AIsBnA3VcAp8cZlEh9yeRdvak0ZFSySaTho+7+oZml7toXTzgi9U/DOEVqFiURfBg2D7mZNQV+QLC+gIiI5IAoTUPXANcTLEZfBhQC6h8QEckRUWoEX3f3i1J3mNmpwKvxhCQiIpkUJRH8Cjgpwj6RBlHTyCCN3hFJr9pEYGanAN8A2pnZjSkvtQHy4g5MJKrykUFVfeBr9I5IejXVCJoR3DtwCNA6Zf92YHScQYnUlkYGiRy8ahOBu/8V+KuZTXH39zMYkyTYwdwApuYfkbqJ0kfwuZlNAroDzct3uvs3Y4tKEqumZp7qqPlHpG6iJILpwNPAcIKhpJcCm+IMSpJNzTwimRXlPoIj3f13wB53/6u7Xw6oNiAikiOi1Aj2hP9uNLOzgQ3AEfGFJCIimRQlEdxrZocDNxHcP9AG+I9YoxIRkYxJmwjc/U/h023AQKi4s1hERHJATTeU5QHnE8wxNNvdV5nZcOB2oAVwYmZClFwQdViohoKKZF5NNYLfAccAi4CHzWwDUATc6u7PZyI4yR1Rh4VqKKhI5tWUCIqAXu7+DzNrDnwEdHb3zZkJTXKNhoWKNE41JYLd7v4PAHffZWbrapsEzGwo8BDB3ESPu/v9VZQ5H7gbcGCFu19Ym/eQxiu1OUhNPiKNV02J4AQzWxk+N6BzuG2Au3uvmk4c9jE8ApwFlAKLzWymu69JKdMFuA041d23mln7OlyLNDKpzUFq8hFpvGpKBF3reO6+wFp3XwdgZk8BI4E1KWW+Czzi7lsB3P2TOr6nNDJqDhJp/GqadK6uE80dDXyYsl0K9KtU5ngAM3uVoPnobnefXflEZnYVcBVAfn5+HcMSEZFUUaaYiNMhQBdgADAW+K2ZfaVyIXef7O5F7l7Url27DIcoIpLb4kwEZQTDT8t1DPelKgVmuvsed38PeJsgMYiISIZEmWICM2sB5Lv7W7U492Kgi5l1IkgAY4DKI4KeJ6gJ/B8za0vQVLSuFu8h9ehg1gKoiUYKiWSHtDUCM/s2sByYHW4XmtnMdMe5+17gBmAO8CbwjLuvNrN7zGxEWGwOsNnM1gAlwDjdp9Bwykf51BeNFBLJDlFqBHcTjACaB+Duy8Nv+Wm5+yxgVqV9d6U8d+DG8CGNgEb5iCRPlD6CPe6+rdI+jyMYERHJvCg1gtVmdiGQF94A9n3gtXjDEhGRTIlSI/gewXrFXwJPEkxHrfUIRERyRJQawQnufgdwR9zBiIhI5kVJBD83s38BngOedvdVMcckGaSJ4UQkbdOQuw8kWJlsE/AbM3vDzMbHHplkROqQUQ33FEmmSDeUuftHBIvTlAA/Au4C7o0zMMkcDRkVSba0icDMugIXAKOAzcDTBAvZSxaqfPewmoNEJEqN4AmCD/8h7r4h5ngkZpWXjFRzkIikTQTurjaDHKOmIBFJVW0iMLNn3P18M3uD/e8kjrRCmTQeGhkkIjWpqUbwg/Df4ZkIROKjJSNFpCY1rVC2MXx6nbvfkvqamf0MuOXAo6SxUnOQiFQnSmfxWRz4of+tKvZJhkVdP0DNQSJSk2pvKDOza8P+ga+b2cqUx3vAysyFKNWJun6AmoNEpCY11QieBP4M/BS4NWX/DnffEmtUEpmafESkrmpKBO7u683s+sovmNkRSgaZU10TkJp8RKQ+pKsRDAeWEgwftZTXHDguxrgkReWbwMqpyUdE6kNNo4aGh/9GWpZS4qUmIBGJS5TF6081s5bh84vN7EEzy48/NBERyYQoK5Q9CnxuZv9KMNncu8DvY41KREQyJkoi2OvuDowEfu3ujwCt4w1LREQyJcoNZTvM7Dbg34H+ZtYEaBpvWCIikilRagQXECxcf3m4QE1HYFKsUYmISMZEWaryI2A6cLiZDQd2ufu02CMTEZGMiDJq6HxgEfBvwPnAQjMbHXdgIiKSGVH6CO4A+rj7JwBm1g74b+C5OAMTEZHMiNJH0KQ8CYQ2RzxORESyQJQawWwzmwMUh9sXALPiC0lERDIpyprF48zsPOC0cNdkd58Rb1i5Ker6AZVpcjkRiVNNaxZ3AR4AOgNvADe7e+0/xaRCdZPHpaPJ5UQkTjXVCJ4ApgHzgW8DvwLOq83JzWwo8BCQBzzu7vdXU24UQedzH3dfUpv3yDaaPE5EGpuaEkFrd/9t+PwtM1tWmxObWR7wCMFSl6XAYjOb6e5rKpVrDfwAWFib84uISP2oKRE0N7MT+ec6BC1St909XWLoC6x193UAZvYUwXxFayqV+zHwM2BcLWMXEZF6UFMi2Ag8mLL9Ucq2A99Mc+6jgQ9TtkuBfqkFzOwk4Bh3f9HMqk0EZnYVcBVAfr5mwBYRqU81LUwzMM43DievexC4LF1Zd58MTAYoKiryOOMSEUmaOG8MKwOOSdnuGO4r1xroAcwzs/XAycBMMyuKMSYREakkzkSwGOhiZp3MrBkwBphZ/qK7b3P3tu5e4O4FwAJgRK6PGhIRaWxiSwTuvhe4AZgDvAk84+6rzeweMxsR1/uKiEjtpL2z2MwMuAg4zt3vCdcr/hd3X5TuWHefRaXpKNz9rmrKDogUsYiI1KsoNYL/DZwCjA23dxDcHyAiIjkgyqRz/dz9JDN7HcDdt4Zt/iIikgOi1Aj2hHcJO1SsR/CPWKMSEZGMiZIIHgZmAO3N7CfA/wPuizUqERHJmCjTUE83s6XAIILpJc5x9zdjj0xERDIiyqihfOBz4I+p+9z9gzgDExGRzIjSWfwiQf+AAc2BTsBbQPcY4xIRkQyJ0jTUM3U7nCjuutgiEhGRjKr1ncXh9NP90hYUEZGsEKWP4MaUzSbAScCG2CISEZGMitJH0Drl+V6CPoM/xBOOiIhkWo2JILyRrLW735yheEREJMOq7SMws0PcfR9wagbjERGRDKupRrCIoD9guZnNBJ4FPit/0d3/K+bYREQkA6L0ETQHNhOsUVx+P4EDSgQiIjmgpkTQPhwxtIp/JoByWjdYRCRH1JQI8oBW7J8AyikRiIjkiJoSwUZ3vydjkYiISIOo6c7iqmoCIiKSY2pKBIMyFoWIiDSYahOBu2/JZCAiItIwaj3pnIiI5BYlAhGRhFMiEBFJuCh3FksaTy78gBeWl6Utt2bjdrp1aJOBiEREolONoB68sLyMNRu3py3XrUMbRhYenYGIRESiU42gnnTr0Ianrz6locMQEak1JYKDlNocpCYfEclmaho6SKnNQWryEZFsphpBHag5SERyQayJwMyGAg8RzGT6uLvfX+n1G4ErCdZC3gRc7u7vxxlTXag5SERyUWxNQ+F6x48A3wK6AWPNrFulYq8DRe7eC3gO+M+44qkPag4SkVwUZ42gL7DW3dcBmNlTwEhgTXkBdy9JKb8AuDjGeOqFmoNEJNfE2Vl8NPBhynZpuK86VwB/ruoFM7vKzJaY2ZJNmzbVY4giItIoRg2Z2cVAETCpqtfdfbK7F7l7Ubt27TIbnIhIjouzaagMOCZlu2O4bz9mdiZwB3CGu38ZYzwiIlKFOGsEi4EuZtbJzJoBY4CZqQXM7ETgN8AId/8kxlhERKQasdUI3H2vmd0AzCEYPvqEu682s3uAJe4+k6ApqBXwrJkBfODuI+KKqTqaNE5EkizW+wjcfRYwq9K+u1Kenxnn+0dVPiw03Ye8hoyKSC7SncUhDQsVkaRqFKOGRESk4SgRiIgknBKBiEjCKRGIiCScEoGISMIpEYiIJJwSgYhIwikRiIgknBKBiEjCKRGIiCScEoGISMIpEYiIJJwSgYhIwikRiIgknBKBiEjCKRGIiCRcYhemSV2eUktQikiSJbZGUL48JWgJShFJtsTWCEDLU4qIQIJrBCIiEkh0jUCkNvbs2UNpaSm7du1q6FBEqtW8eXM6duxI06ZNIx+jRCASUWlpKa1bt6agoAAza+hwRA7g7mzevJnS0lI6deoU+bjEJILUUUKgkUJSe7t27VISkEbNzDjyyCPZtGlTrY5LTB9B6igh0EghOThKAtLYHczfaGJqBKBRQiIiVUlMjUAkF5gZF198ccX23r17adeuHcOHDwdgypQp3HDDDQccV1BQQM+ePenVqxeDBw/mo48+AmDnzp1cffXVdO7cmd69ezNgwAAWLlwIQKtWreot7scee4xp06YB8Pe//53CwkJOPPFE3n33Xb7xjW/U+fyjR49m3bp1FdvLly/HzJg9e3bFvvXr19OjR4/9jrv77rt54IEHKrYfeOABTjjhBAoLC+nTp09FzHUxdepUunTpQpcuXZg6dWq15X71q19xwgkn0L17d370ox8BwQCFSy+9lJ49e9K1a1d++tOfArB7925OP/109u7dW+f4IGE1ApFs17JlS1atWsUXX3xBixYteOmllzj66GhNnCUlJbRt25bbb7+d++67j4cffpgrr7ySTp068c4779CkSRPee+891qxZU+9xX3PNNRXPn3/+eUaPHs348eMBeO211yKfx91xd5o0+ed32NWrV7Nv3z6OO+64in3FxcWcdtppFBcXM3To0Ejnfuyxx3jppZdYtGgRbdq0Yfv27cyYMSNybFXZsmULEydOZMmSJZgZvXv3ZsSIEXz1q1/dr1xJSQkvvPACK1as4NBDD+WTTz4B4Nlnn+XLL7/kjTfe4PPPP6dbt26MHTuWgoICBg0axNNPP81FF11UpxhBiUDkoEz842rWbNievmAtdDuqDRO+3T1tuWHDhvHiiy8yevRoiouLGTt2LK+88krk9zn99NN5+OGHeffdd1m4cCHTp0+v+GDt1KnTAaNNdu7cyciRI9m6dSt79uzh3nvvZeTIkXz22Wecf/75lJaWsm/fPu68804uuOACbr31VmbOnMkhhxzC4MGDeeCBB7j77rtp1aoV3bp145e//CV5eXm8/PLLlJSU0KpVK3bu3AnApEmTeOaZZ/jyyy8599xzmThxIuvXr2fIkCH069ePpUuXMmvWLI499tiK+KZPn87IkSMrtt2dZ599lpdeeon+/fuza9cumjdvnvbnct999zFv3jzatAkGkbRp04ZLL7008s+1KnPmzOGss87iiCOOAOCss85i9uzZjB07dr9yjz76KLfeeiuHHnooAO3btweCGuBnn33G3r17+eKLL2jWrFlFfOeccw633XZbvSQCNQ2JZJkxY8bw1FNPsWvXLlauXEm/fv1qdfyf/vQnevbsyerVqyksLCQvL6/G8s2bN2fGjBksW7aMkpISbrrpJtyd2bNnc9RRR7FixQpWrVrF0KFD2bx5MzNmzGD16tWsXLmy4lt/uWHDhnHNNdfwwx/+kJKSkv1emzt3Lu+88w6LFi1i+fLlLF26lPnz5wPwzjvvcN1117F69er9kgDAq6++Su/evSu2X3vtNTp16kTnzp0ZMGAAL774Ytqfyfbt29mxY8d+tYrqTJo0icLCwgMe3//+9w8oW1ZWxjHHHFOx3bFjR8rKyg4o9/bbb/PKK6/Qr18/zjjjDBYvXgwETV4tW7akQ4cO5Ofnc/PNN1cklR49elSUqyvVCEQOQpRv7nHp1asX69evp7i4mGHDhkU+buDAgeTl5dGrVy/uvffeig/ZdNyd22+/nfnz59OkSRPKysr4+OOP6dmzJzfddBO33HILw4cPp3///uzdu5fmzZtzxRVXMHz48Iq+iyjmzp3L3LlzOfHEE4GgJvLOO++Qn5/Psccey8knn1zlcRs3bqRdu3YV28XFxYwZMwYIkua0adMYNWpUtaNpajvKZty4cYwbN65Wx6Szd+9etmzZwoIFC1i8eDHnn38+69atY9GiReTl5bFhwwa2bt1K//79OfPMMznuuOPIy8ujWbNm7Nixg9atW9fp/WNNBGY2FHgIyAMed/f7K71+KDAN6A1sBi5w9/VxxiSSC0aMGMHNN9/MvHnz2Lx5c6RjyvsIynXv3p0VK1awb9++GmsF06dPZ9OmTSxdupSmTZtSUFDArl27OP7441m2bBmzZs1i/PjxDBo0iLvuuotFixbx8ssv89xzz/HrX/+av/zlL5Hic3duu+02rr766v32r1+/npYtW1Z7XIsWLSru9t63bx9/+MMfeOGFF/jJT35ScYPVjh07OPLII9m6det+x27ZsoVOnTrRpk0bWrVqxbp169LWCiZNmsT06dMP2F/e5Jbq6KOPZt68eRXbpaWlDBgw4IBjO3bsyHnnnYeZ0bdvX5o0acKnn37Kk08+ydChQ2natCnt27fn1FNPZcmSJRUxfvnll5GavdKJrWnIzPKAR4BvAd2AsWbWrVKxK4Ct7v414BfAz+KKRySXXH755UyYMIGePXse9Dk6d+5MUVEREyZMwN2B4EO3clPKtm3baN++PU2bNqWkpIT3338fgA0bNnDYYYdx8cUXM27cOJYtW8bOnTvZtm0bw4YN4xe/+AUrVqyIHM+QIUN44oknKvoLysrKKjpNa9K1a1fWrl0LwMsvv0yvXr348MMPWb9+Pe+//z6jRo1ixowZtGrVig4dOlQkpi1btjB79mxOO+00AG677Tauv/56tm8P+n527txZ5aihcePGsXz58gMelZNA+TXNnTuXrVu3snXrVubOncuQIUMOKHfOOedUNJW9/fbb7N69m7Zt25Kfn18R72effcaCBQs44YQTANi8eTNt27at1VQS1Ymzj6AvsNbd17n7buApYGSlMiOB8vFUzwGDTHfsiKTVsWPHKtukIRhC2rFjx4pHaWlpted5/PHH+fjjj/na175Gjx49uOyyyyo6KstddNFFLFmyhJ49ezJt2rSKD6I33niDvn37UlhYyMSJExk/fjw7duxg+PDh9OrVi9NOO40HH3ww8jUNHjyYCy+8kFNOOYWePXsyevRoduzYkfa4s88+u+Jbd3FxMeeee+5+r48aNYri4mIApk2bxo9//GMKCwv55je/yYQJE+jcuTMA1157LQMHDqRPnz706NGD/v377zc66WAcccQR3HnnnfTp04c+ffpw1113VbTxX3nllSxZsgQIEvu6devo0aMHY8aMYerUqZgZ119/PTt37qR79+706dOH73znO/Tq1QsIanhnn312neIrZ+XfBOqbmY0Ghrr7leH2vwP93P2GlDKrwjKl4fa7YZlPK53rKuAqgPz8/N7l30hqY+IfVwMN27Yr2e3NN9+ka9euDR2GVPLFF18wcOBAXn311bQd37nkvPPO4/777+f4448/4LWq/lbNbKm7F1V1rqzoLHb3ycBkgKKiooPKXEoAIrmpRYsWTJw4kbKyMvLz8xs6nIzYvXs355xzTpVJ4GDEmQjKgGNStjuG+6oqU2pmhwCHE3Qai4hEVlW7ey5r1qwZl1xySb2dL84+gsVAFzPrZGbNgDHAzEplZgLld2yMBv7icbVVidQD/XlKY3cwf6OxJQJ33wvcAMwB3gSecffVZnaPmY0Ii/0OONLM1gI3ArfGFY9IXTVv3pzNmzcrGUijVT5ctrZDSmPrLI5LUVGRl/e0i2SSViiTbFDdCmVZ31ks0hg0bdq0Vqs+iWQLzTUkIpJwSgQiIgmnRCAiknBZ11lsZpuA2t9aHGgLfJq2VG7RNSeDrjkZ6nLNx7p7u6peyLpEUBdmtqS6XvNcpWtOBl1zMsR1zWoaEhFJOCUCEZGES1oimNzQATQAXXMy6JqTIZZrTlQfgYiIHChpNQIREalEiUBEJOFyMhGY2VAze8vM1prZATOamtmhZvZ0+PpCMyvIfJT1K8I132hma8xspZm9bGbHNkSc9SndNaeUG2VmbmZZP9QwyjWb2fnh73q1mT2Z6RjrW4S/7XwzKzGz18O/72ENEWd9MbMnzOyTcAXHql43M3s4/HmsNLOT6vym7p5TDyAPeBc4DmgGrAC6VSpzHfBY+HwM8HRDx52Bax4IHBY+vzYJ1xyWaw3MBxYARQ0ddwZ+z12A14GvhtvtGzruDFzzZODa8Hk3YH1Dx13Haz4dOAlYVc3rw4A/AwacDCys63vmYo2gL7DW3de5+27gKWBkpTIjganh8+eAQWZmGYyxvqW9ZncvcffPw80FBCvGZbMov2eAHwM/A3Jh7ugo1/xd4BF33wrg7p9kOMb6FuWaHWgTPj8c2JDB+Oqdu88HttRQZCQwzQMLgK+YWYe6vGcuJoKjgQ9TtkvDfVWW8WABnW3AkRmJLh5RrjnVFQTfKLJZ2msOq8zHuPuLmQwsRlF+z8cDx5vZq2a2wMyGZiy6eES55ruBi82sFJgFfC8zoTWY2v5/T0vrESSMmV0MFAFnNHQscTKzJsCDwGUNHEqmHULQPDSAoNY338x6uvv/b9Co4jUWmOLuPzezU4Dfm1kPd/9HQweWLXKxRlAGHJOy3THcV2UZMzuEoDq5OSPRxSPKNWNmZwJ3ACPc/csMxRaXdNfcGugBzDOz9QRtqTOzvMM4yu+5FJjp7nvc/T3gbYLEkK2iXPMVwDMA7v43oDnB5Gy5KtL/99rIxUSwGOhiZp3MrBlBZ/DMSmVmApeGz0cDf/GwFyZLpb1mMzsR+A1BEsj2dmNIc83uvs3d27p7gbsXEPSLjHD3bF7nNMrf9vMEtQHMrC1BU9G6TAZZz6Jc8wfAIAAz60qQCDZlNMrMmglcEo4eOhnY5u4b63LCnGsacve9ZnYDMIdgxMET7r7azO4Blrj7TOB3BNXHtQSdMmMaLuK6i3jNk4BWwLNhv/gH7j6iwYKuo4jXnFMiXvMcYLCZrQH2AePcPWtruxGv+Sbgt2b2Q4KO48uy+YudmRUTJPO2Yb/HBKApgLs/RtAPMgxYC3wOfKfO75nFPy8REakHudg0JCIitaBEICKScEoEIiIJp0QgIpJwSgQiIgmnRCCNkpntM7PlKY+CGsrurIf3m2Jm74XvtSy8Q7W253jczLqFz2+v9NprdY0xPE/5z2WVmf3RzL6Spnxhts/GKfHT8FFplMxsp7u3qu+yNZxjCvAnd3/OzAYDD7h7rzqcr84xpTuvmU0F3nb3n9RQ/jKCWVdvqO9YJHeoRiBZwcxahesoLDOzN8zsgJlGzayDmc1P+cbcP9w/2Mz+Fh77rJml+4CeD3wtPPbG8FyrzOw/wn0tzexFM1sR7r8g3D/PzIrM7H6gRRjH9PC1neG/T5nZ2SkxTzGz0WaWZ2aTzGxxOMf81RF+LH8jnGzMzPqG1/i6mb1mZl8P78S9Bytis8wAAAMESURBVLggjOWCMPYnzGxRWLaqGVslaRp67m099KjqQXBX7PLwMYPgLvg24WttCe6qLK/R7gz/vQm4I3yeRzDfUFuCD/aW4f5bgLuqeL8pwOjw+b8BC4HewBtAS4K7slcDJwKjgN+mHHt4+O88wjUPymNKKVMe47nA1PB5M4JZJFsAVwHjw/2HAkuATlXEuTPl+p4FhobbbYBDwudnAn8In18G/Drl+PuAi8PnXyGYi6hlQ/++9WjYR85NMSE54wt3LyzfMLOmwH1mdjrwD4Jvwv8L+CjlmMXAE2HZ5919uZmdQbBYyavh1BrNCL5JV2WSmY0nmKfmCoL5a2a4+2dhDP8F9AdmAz83s58RNCe9Uovr+jPwkJkdCgwF5rv7F2FzVC8zGx2WO5xgsrj3Kh3fwsyWh9f/JvBSSvmpZtaFYJqFptW8/2BghJndHG43B/LDc0lCKRFItrgIaAf0dvc9Fswo2jy1gLvPDxPF2cAUM3sQ2Aq85O5jI7zHOHd/rnzDzAZVVcjd37ZgrYNhwL1m9rK73xPlItx9l5nNA4YAFxAstALBalPfc/c5aU7xhbsXmtlhBPPvXA88TLAAT4m7nxt2rM+r5ngDRrn7W1HilWRQH4Fki8OBT8IkMBA4YM1lC9Zh/tjdfws8TrDc3wLgVDMrb/NvaWbHR3zPV4BzzOwwM2tJ0KzzipkdBXzu7v+XYDK/qtaM3RPWTKryNMFEYeW1Cwg+1K8tP8bMjg/fs0oerDb3feAm++dU6uVTEV+WUnQHQRNZuTnA9yysHlkwK60knBKBZIvpQJGZvQFcAvy9ijIDgBVm9jrBt+2H3H0TwQdjsZmtJGgWOiHKG7r7MoK+g0UEfQaPu/vrQE9gUdhEMwG4t4rDJwMryzuLK5lLsDDQf3uw/CIEiWsNsMyCRct/Q5oaexjLSoKFWf4T+Gl47anHlQDdyjuLCWoOTcPYVofbknAaPioiknCqEYiIJJwSgYhIwikRiIgknBKBiEjCKRGIiCScEoGISMIpEYiIJNz/APtJqDZH9QyLAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHrohQufgh3l",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Regression Evaluation Metrics "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUc_2Fz7gh3m",
        "colab_type": "text"
      },
      "source": [
        "Wine Dataset  \n",
        "    <b> Predictor Variable: </b> Quality (Tells quality of wine)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMZbaFOcgh3o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a61c71e4-da89-4733-a334-e86fa4f32c0b"
      },
      "source": [
        "wine = pd.read_csv(\"https://raw.githubusercontent.com/dphi-official/ML_Models/master/Performance_Evaluation/winequality.csv\", sep=\";\")\n",
        "wine.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.0</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.36</td>\n",
              "      <td>20.7</td>\n",
              "      <td>0.045</td>\n",
              "      <td>45.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>1.0010</td>\n",
              "      <td>3.00</td>\n",
              "      <td>0.45</td>\n",
              "      <td>8.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.3</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.049</td>\n",
              "      <td>14.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.9940</td>\n",
              "      <td>3.30</td>\n",
              "      <td>0.49</td>\n",
              "      <td>9.5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.1</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.40</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0.050</td>\n",
              "      <td>30.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.9951</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.44</td>\n",
              "      <td>10.1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.058</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.058</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n",
              "0            7.0              0.27         0.36  ...       0.45      8.8        6\n",
              "1            6.3              0.30         0.34  ...       0.49      9.5        6\n",
              "2            8.1              0.28         0.40  ...       0.44     10.1        6\n",
              "3            7.2              0.23         0.32  ...       0.40      9.9        6\n",
              "4            7.2              0.23         0.32  ...       0.40      9.9        6\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9dBEH_895Xx",
        "colab_type": "text"
      },
      "source": [
        "### Separate Input and Output Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIuUR4Hy9459",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = wine.drop('quality', axis = 1)\n",
        "y = wine.quality"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP8EUDXTgh3s",
        "colab_type": "text"
      },
      "source": [
        "#### Split into training and testing (80:20)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGgMF3Rzgh3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we are performing both separation of input and output variable and the splitting.\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnuT2aaQgh3z",
        "colab_type": "text"
      },
      "source": [
        "Creating a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsRa3Pe0gh32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PMAIOYRgh4B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7f35c4a9-bf95-4409-d2a8-0039f48e8d29"
      },
      "source": [
        "lr = LinearRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "y_pred = lr.predict(x_test)\n",
        "y_pred[:10]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.44455619, 5.57868309, 5.99091469, 5.19864346, 6.0666099 ,\n",
              "       5.01639077, 5.68416174, 6.26611011, 5.97010538, 5.65519351])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEQuSCzMgh4R",
        "colab_type": "text"
      },
      "source": [
        "## Performance Measurement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FZ86Xpbgh4U",
        "colab_type": "text"
      },
      "source": [
        "Let y = Actual Value,  $\\tilde{y}$ = Predicted Value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjIBhaItgh4X",
        "colab_type": "text"
      },
      "source": [
        "### Mean Absolute Error  \n",
        "*  MAE is the absolute difference between the target value and the value predicted by the model.  \n",
        "*  The MAE is more robust to outliers and does not penalize the errors as extremely as mse\n",
        "\\begin{align}\n",
        "MAE  = \\frac{1}{n}\\sum|y-\\tilde{y}| \n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNOb1yb2gh4Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34c55745-8a56-418b-e2d0-3094280c6179"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(y_test, y_pred)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5972358558776472"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHrz_JZAgh4j",
        "colab_type": "text"
      },
      "source": [
        "### Mean Squared Error\n",
        "*  It is simply the average of the squared difference between the target value and the value predicted by the regression model. \n",
        "*  As it squares the differences, it penalizes even a small error which leads to over-estimation of how bad the model is.\n",
        "*  MSE or Mean Squared Error is one of the most preferred metrics for regression tasks. \n",
        "\\begin{align}\n",
        "MSE & = \\frac{1}{n}\\sum(y-\\tilde{y})^2\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHZcMiDrgh4k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93911077-2b6d-4b9f-d802-4c37c75ad6a5"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Mean Squared Error: \",mean_squared_error(y_test, y_pred))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Squared Error:  0.5906658099548077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx7Dtz5Wgh4r",
        "colab_type": "text"
      },
      "source": [
        "### Root Mean Square Error\n",
        "*  RMSE is the square root of the averaged squared difference between the target value and the value predicted by the model. \n",
        "*  It is preferred more in some cases because the errors are first squared before averaging which poses a high penalty on large errors.\n",
        "*  This implies that RMSE is useful when large errors are undesired.\n",
        "\\begin{align}\n",
        "RMSE  = \\sqrt{\\frac{1}{n}\\sum(y-\\tilde{y})^2}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "EfQI1JFzgh4u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27d0cbec-1f3e-4478-8337-43e846dc27c4"
      },
      "source": [
        "print(\"Root Mean Squared Error: \",mean_squared_error(y_test, y_pred, squared=False))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Root Mean Squared Error:  0.7685478579469256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Onetrj_Agh5A",
        "colab_type": "text"
      },
      "source": [
        "### R Squared\n",
        "<li>The metric helps us to compare our current model with a constant baseline and tells us how much our model is better\n",
        "\\begin{align}\n",
        "R^2 = 1 - \\frac{MSE(Model)}{MSE(Baseline)}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmTpqG2rgh5B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dee569dc-829b-4bd3-9c80-a78670c63190"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test, y_pred)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2832037191111023"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VoVhiVKgh5H",
        "colab_type": "text"
      },
      "source": [
        "# 2. Cross Validation\n",
        "Usually, our data is divided into Train and Test Sets.\n",
        "The Train set is further divided into Train and Validation set. \n",
        "\n",
        "The Validation Set helps us in selecting good parameters/tune the parameters for our model.\n",
        "\n",
        "This Three fold set can be seen in the figure below:\n",
        "\n",
        "![train-test-val](https://amueller.github.io/ml-training-intro/slides/images/threefold_split.png)\n",
        "\n",
        "Our dataset should be as large as possible to train the model and removing considerable part of it for validation poses a problem of losing valuable portion of data that we would prefer to be able to train. \n",
        "\n",
        "In order to address this issue, we use the Cross validation technique. Cross Validation has a number of types out of which we'll be using K-fold cross validation today.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dGdSReX-iFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here the diabetes data is use\n",
        "# separate input and output vairables\n",
        "X = diabetes_data.drop('diabetes', axis = 1)\n",
        "y = diabetes_data.diabetes"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gESg0k8_gh5H",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 K-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKX8Gjyugh5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_validate"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7kl2Ah_gh5Q",
        "colab_type": "text"
      },
      "source": [
        "<blockquote> We can also import cross_val_score from the same library, but it only allows a single scorer to be implemented. So we are using cross_validate </blockquote>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GiU8TBWgh5Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c4a7721f-ee8e-4a58-e8a6-6ffed4a71b10"
      },
      "source": [
        "cv_results = cross_validate(mlp, X, y, cv=10, scoring=[\"accuracy\", \"precision\", \"recall\"])\n",
        "cv_results"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fit_time': array([0.6216476 , 0.26474833, 0.36696339, 0.40454531, 0.50277114,\n",
              "        0.41819572, 0.30417967, 0.45613718, 0.17654657, 0.35016561]),\n",
              " 'score_time': array([0.00448203, 0.00385308, 0.00392938, 0.00409484, 0.0034821 ,\n",
              "        0.0038085 , 0.00380015, 0.00396156, 0.00402236, 0.00363874]),\n",
              " 'test_accuracy': array([0.67532468, 0.68831169, 0.68831169, 0.67532468, 0.7012987 ,\n",
              "        0.75324675, 0.7012987 , 0.7012987 , 0.63157895, 0.69736842]),\n",
              " 'test_precision': array([0.52941176, 0.66666667, 0.5483871 , 0.57142857, 0.66666667,\n",
              "        0.75      , 0.61111111, 0.58333333, 0.46666667, 0.61538462]),\n",
              " 'test_recall': array([0.66666667, 0.22222222, 0.62962963, 0.2962963 , 0.2962963 ,\n",
              "        0.44444444, 0.40740741, 0.51851852, 0.53846154, 0.30769231])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2N7yzQRgh5V",
        "colab_type": "text"
      },
      "source": [
        "**cv=10 is provided, which means we are performing 10 fold cross validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxN-rYmigh5W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0b1f990c-9387-4746-f9be-c9f1dd1aff2b"
      },
      "source": [
        "print(\"Accuracy: \", cv_results[\"test_accuracy\"].mean())\n",
        "print(\"Precision: \", cv_results[\"test_precision\"].mean())\n",
        "print(\"Recall: \", cv_results[\"test_recall\"].mean())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.6913362952836637\n",
            "Precision:  0.6009056492737708\n",
            "Recall:  0.43276353276353274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrNp0R93gh5a",
        "colab_type": "text"
      },
      "source": [
        "**For all valid scoring options - use the following:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mz4G3U5gh5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn.metrics as m\n",
        "m.SCORERS.keys()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygp5tnDdgh5q",
        "colab_type": "text"
      },
      "source": [
        "For more complicated scoring metrics (such as specificity, which isn't explicilty provided by sklearn), or to create your own metrics, \n",
        "http://scikit-learn.org/stable/modules/model_evaluation.html#using-multiple-metric-evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoVnDQsigh5x",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Leave One Out Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDxbxH6ggh5x",
        "colab_type": "text"
      },
      "source": [
        "<blockquote>This code takes a long time to run, you can either skip running this part and directly just see the printed results, or wait for 10-15 mins for this to run </blockquote>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZEbpVaygh5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import LeaveOneOut"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vBvtai8Xgh55",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "86cf758d-e7da-44d6-e566-5138c11c9254"
      },
      "source": [
        "cv_results = cross_validate(mlp, X, y,\n",
        "                            cv=LeaveOneOut(), scoring=[\"accuracy\"])\n",
        "cv_results"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fit_time': array([0.50463486, 0.53072047, 0.45019603, 0.25875711, 0.44739985,\n",
              "        0.35484028, 0.44822884, 0.4314487 , 0.56683135, 0.28208375,\n",
              "        0.50183988, 0.45653701, 0.43308592, 0.3501699 , 0.54133677,\n",
              "        0.40184641, 0.62825012, 0.49153185, 0.42550826, 0.39961076,\n",
              "        0.7513473 , 0.38769817, 0.56053185, 0.38001823, 0.53860569,\n",
              "        0.36647511, 0.36989975, 0.70369744, 0.18759108, 0.79072094,\n",
              "        0.5927453 , 0.29760599, 0.62286425, 0.6358366 , 0.5320394 ,\n",
              "        0.34495783, 0.35085058, 0.3711462 , 0.61121321, 0.37834215,\n",
              "        0.35369539, 0.55936337, 0.33647776, 1.08081794, 0.55860496,\n",
              "        0.31545687, 0.25807309, 0.34375453, 0.6541853 , 0.39383245,\n",
              "        0.42578268, 0.26124644, 0.34834886, 0.30114031, 0.31649947,\n",
              "        0.86546254, 0.36332464, 0.46920919, 0.29200006, 0.53636575,\n",
              "        0.47282386, 0.44233942, 0.40104389, 0.27087355, 0.30107641,\n",
              "        0.28089046, 0.31509233, 0.64117169, 0.38653612, 0.22300935,\n",
              "        0.37118387, 0.40147758, 0.5141418 , 0.37341642, 0.5387094 ,\n",
              "        0.45214701, 0.45951295, 0.22079086, 0.61414099, 0.24913788,\n",
              "        0.24023461, 0.50816917, 0.71763682, 0.39898467, 0.77164221,\n",
              "        0.30258107, 0.42391968, 0.53649569, 0.2894702 , 0.29695702,\n",
              "        0.51310349, 0.34438348, 0.49352527, 0.32067084, 0.29145384,\n",
              "        0.36725187, 0.35692358, 0.64930987, 0.20785499, 0.51301527,\n",
              "        0.33421779, 0.665066  , 0.37500238, 0.37442088, 0.51687169,\n",
              "        0.52342224, 0.66150045, 0.37497091, 0.31103611, 0.51466656,\n",
              "        0.62332416, 0.39307237, 0.29637742, 0.6456418 , 0.41671515,\n",
              "        0.67102909, 0.604002  , 0.41557932, 0.33163476, 0.2853179 ,\n",
              "        0.34255552, 0.27735424, 0.31792045, 0.34699631, 0.53410745,\n",
              "        0.33766437, 0.42309189, 0.26876497, 0.74222326, 0.24560785,\n",
              "        0.49373126, 0.4339354 , 0.52147198, 0.34392047, 0.28127098,\n",
              "        0.38436675, 0.31272817, 0.53037453, 0.5374856 , 0.27603769,\n",
              "        0.19200587, 0.61506772, 0.34487247, 0.5391078 , 0.55426502,\n",
              "        0.24297833, 0.43219972, 0.49298644, 0.32329583, 0.36117625,\n",
              "        0.35421991, 0.44792008, 0.59881449, 0.41914344, 0.50238276,\n",
              "        0.63895988, 0.28849602, 0.40206265, 0.38545752, 0.38214731,\n",
              "        0.29024935, 0.31217146, 0.43061256, 0.4776113 , 0.40388465,\n",
              "        0.51507998, 0.5087738 , 0.34355903, 0.39496517, 0.34957576,\n",
              "        0.35211682, 0.56584668, 0.51624298, 0.19680548, 0.51801896,\n",
              "        0.65368533, 0.35921574, 0.25274205, 0.78581452, 0.48367977,\n",
              "        0.50630903, 0.27502441, 0.73099804, 0.634902  , 0.54477978,\n",
              "        0.28181505, 0.43753219, 0.35684085, 0.47675848, 0.25811172,\n",
              "        0.48628259, 0.51649976, 0.49593091, 0.34844828, 0.17254019,\n",
              "        0.44043875, 0.45224667, 0.53328514, 0.18131375, 0.48274183,\n",
              "        0.5220716 , 0.21085024, 0.44751191, 0.35901451, 0.2980845 ,\n",
              "        0.70825982, 0.33583879, 0.5664053 , 0.33390021, 0.57570839,\n",
              "        0.3993938 , 0.65876126, 0.59888005, 0.35638714, 0.38827848,\n",
              "        0.38218379, 0.41527581, 0.44477963, 0.36143136, 0.36965418,\n",
              "        0.52363133, 0.30174994, 0.53791857, 0.55321383, 0.38329577,\n",
              "        0.25894189, 0.65577841, 0.34727716, 0.30907226, 0.50339794,\n",
              "        0.55806184, 0.47379494, 0.46235871, 0.43697405, 0.52072835,\n",
              "        0.34017301, 0.45354414, 0.29178524, 0.33620691, 0.52031636,\n",
              "        0.24597526, 0.47953367, 0.29677367, 0.38410234, 0.39189982,\n",
              "        0.81251502, 0.25562382, 0.34686327, 0.6027112 , 0.30490279,\n",
              "        0.3809123 , 0.74501467, 0.60324883, 0.49031091, 0.38930893,\n",
              "        0.4318738 , 0.30355597, 0.46014524, 0.60598397, 0.34713268,\n",
              "        0.49728012, 0.31839395, 0.37784934, 0.41048646, 0.25599718,\n",
              "        0.58802319, 0.62212229, 0.53347015, 0.35016799, 0.56488657,\n",
              "        0.43056607, 0.24214745, 0.46073198, 0.34499264, 0.38736081,\n",
              "        0.35053539, 0.59475636, 0.29971528, 0.61331749, 0.24699998,\n",
              "        0.49333763, 0.38630915, 0.30499172, 0.33025217, 0.72953606,\n",
              "        0.6706953 , 0.18211818, 0.27373457, 0.58337069, 0.49481869,\n",
              "        0.3814981 , 0.29272771, 0.23603821, 0.62341881, 0.32674384,\n",
              "        0.49381185, 0.32868242, 0.27909112, 0.3039844 , 0.43001318,\n",
              "        0.57917666, 0.32847357, 0.63422894, 0.82804894, 0.69510388,\n",
              "        0.32787275, 0.40534711, 0.42204809, 0.45027208, 0.5298574 ,\n",
              "        0.6024332 , 0.26918387, 0.54141855, 0.28917789, 0.45354342,\n",
              "        0.76994801, 0.28853416, 1.05008197, 0.18520141, 0.28974652,\n",
              "        0.5082643 , 0.35846043, 0.25214267, 0.25392056, 0.32605505,\n",
              "        0.66925097, 0.41664934, 0.63350654, 0.42101312, 0.31841707,\n",
              "        0.59753871, 0.56682348, 0.45055699, 0.40690446, 0.63402843,\n",
              "        0.74538827, 0.46316957, 0.31213236, 0.26298046, 0.3919549 ,\n",
              "        0.46567869, 0.42785811, 0.32431316, 0.46483088, 0.52842951,\n",
              "        0.31808186, 0.43860722, 0.46780658, 0.47820473, 0.17578864,\n",
              "        0.34874439, 0.54525781, 0.33986044, 0.29775667, 0.51880908,\n",
              "        0.41012168, 0.22243237, 0.31190991, 0.37900782, 0.30549026,\n",
              "        0.43425846, 0.46504283, 0.323982  , 0.48053479, 0.65179038,\n",
              "        0.47189927, 0.34780931, 0.45932937, 0.37880182, 0.45390248,\n",
              "        0.4021883 , 0.42636442, 0.46431828, 0.45582771, 0.29675651,\n",
              "        0.46220255, 0.85413074, 0.34656525, 0.34374928, 0.48029876,\n",
              "        0.51164889, 0.30534053, 0.38759089, 0.29838133, 0.48368287,\n",
              "        0.32666564, 0.3996532 , 0.3467145 , 0.4147048 , 0.46770835,\n",
              "        0.53640723, 0.69266415, 0.28718662, 0.50355697, 0.37993407,\n",
              "        0.61812091, 0.49330759, 0.36579275, 0.36205959, 0.42425799,\n",
              "        0.46663427, 0.35737848, 0.40506363, 0.80543041, 0.29585314,\n",
              "        0.31420422, 0.27206898, 0.45290828, 0.20518756, 0.43403077,\n",
              "        0.26344514, 0.48691154, 0.36908436, 0.33660555, 0.60255408,\n",
              "        0.55091763, 0.43488359, 0.85036159, 0.33292389, 0.608639  ,\n",
              "        0.30101991, 0.6149931 , 0.4948895 , 0.21885085, 0.43819785,\n",
              "        0.22187138, 0.42483592, 0.43421817, 0.53345394, 0.54092336,\n",
              "        0.46509457, 0.39948297, 0.49952793, 0.29318452, 0.78769445,\n",
              "        0.32186937, 0.33737016, 0.55848503, 0.32469893, 0.51953673,\n",
              "        0.59349656, 0.6791842 , 0.41095757, 0.58685803, 0.99464893,\n",
              "        0.49655628, 0.32202411, 0.27545786, 0.36232758, 0.42216492,\n",
              "        0.45616221, 0.43600225, 0.50798965, 0.41601539, 0.5014112 ,\n",
              "        0.52832818, 0.53271508, 0.39432526, 0.49213028, 0.52631044,\n",
              "        0.57215405, 0.47402644, 0.44019413, 0.48415971, 0.5585115 ,\n",
              "        0.85720706, 0.46575451, 0.47046041, 0.447613  , 0.32152748,\n",
              "        0.61787772, 0.32988977, 0.65758848, 0.32352686, 0.30150247,\n",
              "        0.35401511, 0.43204212, 0.41048646, 0.51850939, 0.42013931,\n",
              "        0.35896969, 0.41403079, 0.45184851, 0.4856863 , 0.30005908,\n",
              "        0.24561167, 0.4011848 , 0.24178267, 0.33183575, 0.49329829,\n",
              "        0.39002347, 0.70686746, 0.63224816, 0.47435713, 0.35869122,\n",
              "        0.31593156, 0.24692488, 0.98483109, 0.43657446, 0.19653392,\n",
              "        0.3525877 , 0.24830866, 0.44002104, 0.32349038, 0.61934018,\n",
              "        0.40117264, 0.61115122, 0.57803798, 0.50183582, 0.4164257 ,\n",
              "        0.38922548, 0.39308882, 0.55514312, 0.32309818, 0.49016333,\n",
              "        0.27060127, 0.40628719, 0.35774565, 0.31511593, 0.5431993 ,\n",
              "        0.52988935, 0.42989182, 0.25176191, 0.68284321, 0.24662399,\n",
              "        0.41436005, 0.45106912, 0.45901728, 0.1848433 , 0.50998163,\n",
              "        0.36007404, 0.66101742, 0.33320832, 0.88560224, 0.94355345,\n",
              "        0.25007844, 0.44286752, 0.70518851, 0.8595984 , 0.49749041,\n",
              "        0.31023788, 0.40848041, 0.59849668, 0.54552674, 0.44356608,\n",
              "        0.29299593, 0.24653006, 0.50384498, 0.33561254, 0.23134446,\n",
              "        0.36153293, 0.42351675, 0.59542918, 0.65259719, 0.25840521,\n",
              "        0.37711692, 0.44712472, 0.30049825, 0.35673642, 0.77378273,\n",
              "        0.34893465, 0.42081141, 0.47364616, 0.54364014, 0.38431525,\n",
              "        0.42776823, 0.37409353, 0.30416417, 0.2216351 , 0.32948995,\n",
              "        0.3681941 , 0.39223719, 0.38955712, 0.20906711, 0.66548276,\n",
              "        0.27816653, 0.33243513, 0.449404  , 0.4205482 , 0.44686937,\n",
              "        0.47231722, 0.45458174, 0.25368834, 0.4643116 , 0.60249138,\n",
              "        0.41990805, 0.58057117, 0.32751656, 0.47703218, 0.70087075,\n",
              "        0.247859  , 0.6592865 , 0.54389524, 0.35507941, 0.45954108,\n",
              "        0.24983358, 0.30664086, 0.47671866, 0.6186924 , 0.49534011,\n",
              "        0.37733722, 0.40667439, 0.2894671 , 0.21074915, 0.61685014,\n",
              "        0.39772367, 0.76578283, 0.56362057, 0.40363431, 0.27987909,\n",
              "        0.27369475, 0.35851312, 0.40671921, 0.49610519, 0.50181603,\n",
              "        0.61846828, 0.4686656 , 0.52519774, 0.42417693, 0.48592806,\n",
              "        0.1942637 , 0.46465707, 0.44046712, 0.25837517, 0.45570517,\n",
              "        0.45590949, 0.21748781, 0.63027263, 0.47238302, 0.49657345,\n",
              "        0.67364478, 0.36631656, 0.38182545, 0.62530541, 0.19101477,\n",
              "        0.45278454, 0.66529298, 0.36894965, 0.3225472 , 0.41417575,\n",
              "        0.48247123, 0.39958358, 0.50557208, 0.44034171, 0.46053195,\n",
              "        0.36297107, 0.47267675, 0.3582046 , 0.22216439, 0.50507784,\n",
              "        0.46169686, 0.35528803, 0.58199263, 0.84210062, 0.35550833,\n",
              "        0.64160132, 0.36476398, 0.55987835, 0.29665589, 0.3396349 ,\n",
              "        0.26309872, 0.63995314, 0.4065733 , 0.75952768, 0.48121786,\n",
              "        0.53994632, 0.42755246, 0.42635965, 0.36551976, 0.43276954,\n",
              "        0.41230845, 0.67100525, 0.45673656, 0.494627  , 0.44857645,\n",
              "        0.43146706, 0.64419413, 0.59578347, 0.19735003, 0.38629842,\n",
              "        0.692904  , 0.75912189, 0.847507  , 0.602988  , 0.69907975,\n",
              "        0.27635503, 0.48486614, 0.48711205, 0.38594866, 0.22173214,\n",
              "        0.53533697, 0.49416447, 0.51740861, 0.21388626, 0.4450264 ,\n",
              "        0.41731381, 0.45918465, 0.3890481 , 0.32813811, 0.30831504,\n",
              "        0.63626599, 0.4092989 , 0.3986938 , 0.41163135, 0.36881638,\n",
              "        0.74993658, 0.37026477, 0.31862545, 0.34085703, 0.65305495,\n",
              "        0.54285789, 0.45775008, 0.65603185, 0.25762367, 0.34399414,\n",
              "        0.23592758, 0.25760365, 0.34459162, 0.52925992, 0.72668481,\n",
              "        0.79731369, 0.2086277 , 0.25003099, 0.28380823, 0.87957287,\n",
              "        0.6245575 , 0.49064827, 0.33846545, 0.58542728, 0.40965009,\n",
              "        0.17214131, 0.45953822, 0.68598461, 0.30573845, 0.41018605,\n",
              "        0.24846697, 0.20160365, 0.27820468, 0.47728181, 0.69597149,\n",
              "        0.46702647, 0.61612535, 0.54049993, 0.21504641, 0.36993623,\n",
              "        0.60263515, 0.49927974, 0.50468659, 0.34509134, 0.56071234,\n",
              "        0.56335998, 0.75145459, 0.19760871, 0.39144468, 0.72952318,\n",
              "        0.45775199, 0.58661914, 0.61249614, 0.37444806, 0.26316428,\n",
              "        0.57093978, 0.42567253, 0.37494779, 0.60145903, 0.32830334,\n",
              "        0.43621564, 0.42065859, 0.65161371]),\n",
              " 'score_time': array([0.00167298, 0.00149179, 0.00170612, 0.00158763, 0.0015378 ,\n",
              "        0.00154066, 0.00156331, 0.0017767 , 0.0015285 , 0.00166893,\n",
              "        0.00149655, 0.00150132, 0.00177574, 0.00153971, 0.0015316 ,\n",
              "        0.00173712, 0.00174451, 0.00146127, 0.00155425, 0.00150752,\n",
              "        0.00148869, 0.00158477, 0.0017662 , 0.00150323, 0.00152349,\n",
              "        0.00159383, 0.00145364, 0.00170279, 0.00165081, 0.00156999,\n",
              "        0.00157714, 0.00156116, 0.00150847, 0.00155449, 0.00170684,\n",
              "        0.00147915, 0.00155067, 0.00176334, 0.00155234, 0.00199485,\n",
              "        0.00161076, 0.00150418, 0.00158739, 0.00151062, 0.0015049 ,\n",
              "        0.00164533, 0.00158262, 0.00170827, 0.00151014, 0.00155234,\n",
              "        0.00175023, 0.0015173 , 0.00148034, 0.00151944, 0.00149465,\n",
              "        0.0015192 , 0.00172997, 0.00193262, 0.00144053, 0.00170159,\n",
              "        0.00164413, 0.00148702, 0.00158095, 0.00161719, 0.00161004,\n",
              "        0.00152922, 0.00168276, 0.00165749, 0.00157428, 0.00156522,\n",
              "        0.00149584, 0.00157452, 0.00163436, 0.00158834, 0.0015471 ,\n",
              "        0.00151086, 0.00175643, 0.00151372, 0.00177431, 0.00165725,\n",
              "        0.00160122, 0.00148249, 0.00153422, 0.00152254, 0.00177097,\n",
              "        0.00159717, 0.00169182, 0.00166512, 0.00176311, 0.00167155,\n",
              "        0.00171351, 0.00152087, 0.00161958, 0.00165176, 0.00152731,\n",
              "        0.00155377, 0.00160575, 0.00150681, 0.00158453, 0.00175595,\n",
              "        0.00158215, 0.00160456, 0.00397134, 0.00166655, 0.00164127,\n",
              "        0.0019114 , 0.00167799, 0.00151706, 0.00177717, 0.00168061,\n",
              "        0.00167108, 0.00153422, 0.00160623, 0.00167489, 0.00159192,\n",
              "        0.00167131, 0.00157762, 0.00163674, 0.00157452, 0.0015161 ,\n",
              "        0.00146222, 0.00151587, 0.00154924, 0.00174451, 0.0016861 ,\n",
              "        0.00153804, 0.00159812, 0.00178838, 0.00147843, 0.00162816,\n",
              "        0.0015564 , 0.00156665, 0.00159192, 0.00144172, 0.00154781,\n",
              "        0.00154853, 0.00161672, 0.00148654, 0.00145864, 0.00162864,\n",
              "        0.00168252, 0.00157332, 0.00169802, 0.0016489 , 0.00158429,\n",
              "        0.00168681, 0.00162458, 0.00161958, 0.00147009, 0.00154376,\n",
              "        0.00144339, 0.00155115, 0.00176668, 0.00151467, 0.0015769 ,\n",
              "        0.00155592, 0.00167203, 0.0015626 , 0.00170612, 0.00146556,\n",
              "        0.00151229, 0.00163174, 0.00154042, 0.00153351, 0.0017972 ,\n",
              "        0.0015173 , 0.0017333 , 0.00157952, 0.00147605, 0.001508  ,\n",
              "        0.00155449, 0.00163245, 0.00160241, 0.00154543, 0.00157809,\n",
              "        0.00161338, 0.00160789, 0.00150204, 0.00163531, 0.00173926,\n",
              "        0.00177336, 0.00157118, 0.00164223, 0.00159335, 0.0014627 ,\n",
              "        0.00150323, 0.00154114, 0.00150156, 0.00155473, 0.00163198,\n",
              "        0.0017457 , 0.0017364 , 0.0014863 , 0.00154829, 0.001477  ,\n",
              "        0.00181246, 0.00169754, 0.00160265, 0.00171638, 0.0017724 ,\n",
              "        0.00161004, 0.00157785, 0.00220418, 0.00162101, 0.00176454,\n",
              "        0.00165319, 0.00149989, 0.00145245, 0.00156212, 0.00159597,\n",
              "        0.00158906, 0.00157475, 0.00145149, 0.0014534 , 0.00161767,\n",
              "        0.00162339, 0.00155592, 0.00149179, 0.00144672, 0.00149584,\n",
              "        0.00154018, 0.00145054, 0.00146174, 0.00154924, 0.00170779,\n",
              "        0.0016458 , 0.00168371, 0.00159955, 0.00170469, 0.00155592,\n",
              "        0.00169516, 0.00158   , 0.00153708, 0.00152373, 0.00158358,\n",
              "        0.00162125, 0.00146508, 0.00170684, 0.00176454, 0.00154591,\n",
              "        0.00166535, 0.00152779, 0.00176954, 0.00147057, 0.00156331,\n",
              "        0.00149107, 0.00149941, 0.00154924, 0.00170922, 0.00147486,\n",
              "        0.00174022, 0.0015161 , 0.00180554, 0.00171137, 0.00153899,\n",
              "        0.0015707 , 0.00148964, 0.00167441, 0.00156045, 0.0015645 ,\n",
              "        0.00156832, 0.00163817, 0.00155735, 0.00181508, 0.00149465,\n",
              "        0.00156474, 0.00158119, 0.00165629, 0.00174332, 0.00168252,\n",
              "        0.00156593, 0.00146675, 0.00167489, 0.00152707, 0.00146079,\n",
              "        0.00160694, 0.00148606, 0.0014596 , 0.0016346 , 0.0016191 ,\n",
              "        0.00169921, 0.00151539, 0.00147414, 0.00169373, 0.00161767,\n",
              "        0.00155783, 0.00157332, 0.00152588, 0.00147867, 0.00150299,\n",
              "        0.0015645 , 0.0014801 , 0.00146604, 0.00160313, 0.00156212,\n",
              "        0.00161886, 0.00169039, 0.0015583 , 0.00168729, 0.00146508,\n",
              "        0.00158715, 0.0015471 , 0.00164247, 0.0015893 , 0.0016551 ,\n",
              "        0.00173211, 0.00156665, 0.0016346 , 0.00165772, 0.00161386,\n",
              "        0.0017581 , 0.00170588, 0.00154686, 0.00147891, 0.00153995,\n",
              "        0.00150847, 0.00154901, 0.00168157, 0.0014832 , 0.00173235,\n",
              "        0.00176477, 0.00162268, 0.00153375, 0.00161052, 0.00174189,\n",
              "        0.00159431, 0.00171995, 0.00156236, 0.00175738, 0.0017128 ,\n",
              "        0.00173974, 0.0017333 , 0.00147629, 0.00144458, 0.00162506,\n",
              "        0.00154376, 0.00177097, 0.0014751 , 0.00161076, 0.00155902,\n",
              "        0.00173259, 0.00158   , 0.00173593, 0.00171304, 0.00146961,\n",
              "        0.00154853, 0.00174451, 0.00171018, 0.00156879, 0.00148153,\n",
              "        0.0015955 , 0.00165439, 0.00172019, 0.00172257, 0.00156498,\n",
              "        0.0015099 , 0.00155711, 0.00143337, 0.00162077, 0.00146484,\n",
              "        0.00168228, 0.00153518, 0.00171995, 0.0016768 , 0.00166583,\n",
              "        0.00330806, 0.00150561, 0.00158834, 0.00175023, 0.00152493,\n",
              "        0.00167775, 0.00176215, 0.00150633, 0.0014658 , 0.00147104,\n",
              "        0.00148726, 0.00164199, 0.0016737 , 0.00163913, 0.00161123,\n",
              "        0.00166464, 0.00169325, 0.00153041, 0.00152993, 0.00173044,\n",
              "        0.00157809, 0.00167203, 0.00173163, 0.00154448, 0.00150084,\n",
              "        0.00148845, 0.00184488, 0.00177217, 0.00145984, 0.00161982,\n",
              "        0.00159717, 0.00156832, 0.00167656, 0.00155663, 0.00148892,\n",
              "        0.0025537 , 0.00152516, 0.0017345 , 0.00165486, 0.00157928,\n",
              "        0.00155234, 0.00160551, 0.00156474, 0.0016768 , 0.00170612,\n",
              "        0.00156784, 0.0016067 , 0.00166821, 0.00158787, 0.00156522,\n",
              "        0.00176835, 0.00162697, 0.00173759, 0.00150776, 0.00144958,\n",
              "        0.00153184, 0.00152707, 0.0015955 , 0.00149822, 0.00147629,\n",
              "        0.00150323, 0.0014708 , 0.00161934, 0.00152802, 0.00167036,\n",
              "        0.00160933, 0.00187969, 0.00157809, 0.00166106, 0.0014565 ,\n",
              "        0.0017457 , 0.00162745, 0.0015521 , 0.00173378, 0.00158095,\n",
              "        0.00164747, 0.00161362, 0.00167656, 0.00157309, 0.00158072,\n",
              "        0.0016129 , 0.00159073, 0.00170779, 0.00153255, 0.00154638,\n",
              "        0.00158763, 0.00149226, 0.00152683, 0.00161052, 0.0015769 ,\n",
              "        0.00150704, 0.00163579, 0.00165033, 0.00153375, 0.00156522,\n",
              "        0.00161624, 0.00160956, 0.00163841, 0.00152421, 0.00155997,\n",
              "        0.00158405, 0.00172687, 0.00166249, 0.00154305, 0.00156069,\n",
              "        0.00158787, 0.00162458, 0.00167847, 0.00157571, 0.00157475,\n",
              "        0.00162673, 0.00188541, 0.00159979, 0.00153828, 0.00161052,\n",
              "        0.00166726, 0.00164866, 0.00156379, 0.0015192 , 0.00152612,\n",
              "        0.00159979, 0.00181484, 0.00183964, 0.00162244, 0.00153613,\n",
              "        0.0016017 , 0.0017364 , 0.00174356, 0.00158715, 0.00159931,\n",
              "        0.00168347, 0.00183058, 0.00172877, 0.0016222 , 0.00176597,\n",
              "        0.00171208, 0.00157809, 0.00148845, 0.00152612, 0.00164652,\n",
              "        0.00156474, 0.00156522, 0.00152707, 0.00155377, 0.00163841,\n",
              "        0.00153565, 0.00156879, 0.00170875, 0.00149822, 0.00150084,\n",
              "        0.00169301, 0.00171161, 0.00163317, 0.00153136, 0.00151443,\n",
              "        0.00156879, 0.00161648, 0.00177145, 0.00150084, 0.00176382,\n",
              "        0.00168276, 0.00162387, 0.00147605, 0.00158405, 0.00151014,\n",
              "        0.00147843, 0.00166845, 0.00150871, 0.00167632, 0.00174809,\n",
              "        0.00154972, 0.00145411, 0.00154948, 0.00154901, 0.00146866,\n",
              "        0.00162721, 0.00166893, 0.00166416, 0.00163007, 0.00149727,\n",
              "        0.0017395 , 0.00158978, 0.00154424, 0.0016799 , 0.00155997,\n",
              "        0.00162578, 0.00150585, 0.00159454, 0.0015974 , 0.00174069,\n",
              "        0.0017345 , 0.00164866, 0.00165319, 0.00179839, 0.00144696,\n",
              "        0.00147414, 0.00154042, 0.00163579, 0.00167537, 0.00173783,\n",
              "        0.00170684, 0.00164366, 0.00159812, 0.00148177, 0.00156331,\n",
              "        0.00167203, 0.00153685, 0.00154138, 0.00155878, 0.00161386,\n",
              "        0.00176883, 0.00173211, 0.00178123, 0.00156808, 0.00151205,\n",
              "        0.00150156, 0.00153828, 0.00153542, 0.00152063, 0.00159168,\n",
              "        0.00157619, 0.00154066, 0.00150943, 0.00152326, 0.00178862,\n",
              "        0.00179625, 0.0016706 , 0.00168133, 0.00161767, 0.00156236,\n",
              "        0.00175691, 0.00155401, 0.00150585, 0.00144005, 0.0016551 ,\n",
              "        0.00154877, 0.00145745, 0.00155163, 0.00165534, 0.00150609,\n",
              "        0.00144482, 0.00158072, 0.00156331, 0.0016253 , 0.00183678,\n",
              "        0.00155616, 0.00151944, 0.00154114, 0.00162268, 0.00172901,\n",
              "        0.00173402, 0.00169277, 0.00154448, 0.00154495, 0.00788784,\n",
              "        0.00151706, 0.0016408 , 0.00144815, 0.00148296, 0.00154376,\n",
              "        0.00148988, 0.0015583 , 0.00185752, 0.00166774, 0.00147963,\n",
              "        0.00178885, 0.00166035, 0.00171971, 0.00152683, 0.0016346 ,\n",
              "        0.00169086, 0.00166702, 0.00173044, 0.00161147, 0.00166774,\n",
              "        0.00163746, 0.00170708, 0.00151086, 0.00171399, 0.00154305,\n",
              "        0.00180101, 0.00156498, 0.00152254, 0.00176358, 0.00164199,\n",
              "        0.00171375, 0.00161552, 0.00162053, 0.00166774, 0.0016377 ,\n",
              "        0.00165534, 0.00154018, 0.0014925 , 0.0016861 , 0.00176287,\n",
              "        0.00150537, 0.00155377, 0.00155449, 0.00153995, 0.00150704,\n",
              "        0.00146532, 0.00168729, 0.00154495, 0.00179291, 0.00156021,\n",
              "        0.00179267, 0.00168133, 0.00174332, 0.00163579, 0.00173378,\n",
              "        0.0015316 , 0.00174332, 0.00171947, 0.00172329, 0.00160074,\n",
              "        0.00156689, 0.00157881, 0.00158858, 0.00180507, 0.00171804,\n",
              "        0.00177026, 0.00157285, 0.0015676 , 0.00159979, 0.00153446,\n",
              "        0.00164437, 0.00155926, 0.00154877, 0.00163579, 0.0015099 ,\n",
              "        0.00160003, 0.00163102, 0.00172019, 0.00168347, 0.00177932,\n",
              "        0.00176167, 0.00176692, 0.00164223, 0.00158668, 0.0016048 ,\n",
              "        0.00173092, 0.00169802, 0.00170112, 0.00157571, 0.00172162,\n",
              "        0.00154877, 0.00157952, 0.00162768, 0.00159025, 0.00160956,\n",
              "        0.00178599, 0.00164342, 0.00169969, 0.00152469, 0.0016706 ,\n",
              "        0.00171733, 0.00160551, 0.00172234, 0.00156808, 0.00154591,\n",
              "        0.00164008, 0.00153375, 0.00172091, 0.00170708, 0.00163078,\n",
              "        0.00157094, 0.00151539, 0.0016973 , 0.00164223, 0.0016675 ,\n",
              "        0.00158024, 0.0016458 , 0.00172877, 0.0017395 , 0.00154972,\n",
              "        0.00179243, 0.00158501, 0.00186133, 0.00161028, 0.00167513,\n",
              "        0.00156426, 0.00174189, 0.00154352, 0.00175881, 0.00166869,\n",
              "        0.00173664, 0.0016439 , 0.00156355, 0.00149918, 0.00177193,\n",
              "        0.001683  , 0.00160241, 0.00160742, 0.00172806, 0.00166893,\n",
              "        0.00172853, 0.00149345, 0.00155473, 0.00157833, 0.00180173,\n",
              "        0.00181699, 0.00175858, 0.0017736 ]),\n",
              " 'test_accuracy': array([0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
              "        0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
              "        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
              "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
              "        1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
              "        1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
              "        1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
              "        0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
              "        0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
              "        0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
              "        0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
              "        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
              "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
              "        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
              "        1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
              "        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
              "        0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
              "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
              "        1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
              "        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
              "        0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
              "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
              "        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
              "        0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
              "        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
              "        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
              "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
              "        0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
              "        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
              "        0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
              "        1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
              "        1., 0., 1.])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VXdvPbqgh6F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "936c9d9c-426c-499f-d22a-44bcee1d4717"
      },
      "source": [
        "cv_results['test_accuracy'].mean()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7252604166666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh7CwmWygh6b",
        "colab_type": "text"
      },
      "source": [
        "We have not included precision and recall in the metrics here. Can you think why?  \n",
        "**<mark>Hint:</mark> Imagine the confusion matrix when the testing has only one sample**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1GyHfC0gh6j",
        "colab_type": "text"
      },
      "source": [
        "## 3. Hyperparameter Tuning\n",
        "Hyperparameters are important parts of the ML model and can make the model gold or trash. Here we have discussed one of the popular hyperparameter tunning method i.e. using Grid Search CV. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEHcCWcegh6l",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Grid Search CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQwxdMUkgh6m",
        "colab_type": "text"
      },
      "source": [
        "### 3.1.1 Crime Rate- Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qVEuJimgh6n",
        "colab_type": "text"
      },
      "source": [
        "**Predictor Variable: Crime Rate (Regression Based)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Wbh15ZTgh6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NamrnJjOgh6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "crime = pd.read_csv(\"https://raw.githubusercontent.com/dphi-official/ML_Models/master/Performance_Evaluation/Standard%20Metropolitan%20Areas%20Data%20-%20train_data.csv\")\n",
        "train, test = train_test_split(crime)\n",
        "x_train = train.drop('crime_rate', axis = 1)     \n",
        "y_train = train.crime_rate      \n",
        "x_test = test.drop('crime_rate', axis = 1)      \n",
        "y_test = test.crime_rate        "
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DZvhZ0fgh66",
        "colab_type": "text"
      },
      "source": [
        "Performance without grid search: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TjevVRegh67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "y_pred = lr.predict(x_test)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2rzRQoBgh7D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5af611ce-027b-4ebb-dd2a-82590614b5fa"
      },
      "source": [
        "mean_squared_error(y_test, y_pred, squared=False)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.438949932950381"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no8Vw1Qmgh7N",
        "colab_type": "text"
      },
      "source": [
        "Performance with Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UPswE7Ogh7O",
        "colab_type": "text"
      },
      "source": [
        "**Step 1:** Define a parameter Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6084pRAvgh7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False], 'n_jobs':[-1,1,10,15]}"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPQl8cdHgh7Y",
        "colab_type": "text"
      },
      "source": [
        "**Step 2:** Fit the model to find the best hyperparameters on training data, and select the scorer you want to select to optimise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSWCh9rDgh7Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "49a8d730-a8ee-4b0f-aeb4-ccfab7380562"
      },
      "source": [
        "grid = GridSearchCV(lr,parameters, cv=3)\n",
        "grid.fit(x_train, y_train)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, error_score=nan,\n",
              "             estimator=LinearRegression(copy_X=True, fit_intercept=True,\n",
              "                                        n_jobs=None, normalize=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'copy_X': [True, False],\n",
              "                         'fit_intercept': [True, False],\n",
              "                         'n_jobs': [-1, 1, 10, 15],\n",
              "                         'normalize': [True, False]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTdZBb40gh7d",
        "colab_type": "text"
      },
      "source": [
        "**Step 3:** Print the best obtained parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIonXbIogh7f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8d7b1d6-5863-4d06-92c7-b50a8aa1d6b4"
      },
      "source": [
        "grid.best_estimator_"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Urv2jtngh7n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "59100aaa-999d-4a36-8dce-0654edb6f1d3"
      },
      "source": [
        "grid_lr = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False)\n",
        "grid_lr.fit(x_train, y_train)\n",
        "y_pred= grid_lr.predict(x_test)\n",
        "mean_squared_error(y_test, y_pred, squared=False)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.438949932950381"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwhEjnmGgh7v",
        "colab_type": "text"
      },
      "source": [
        "**Performance does not vary that much!**\n",
        "\n",
        "The number of hyperparameters for Linear Regression is very less. Hence all of them give similar performance (in this specific dataset)\n",
        "\n",
        "Let us try another parameter for which the performance varies a lot!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gp-Po0wgh7w",
        "colab_type": "text"
      },
      "source": [
        "### 3.1.2 Artificial Neural Network\n",
        "In Linear Regression, there are not many parameters to optimise, hence performance may not vary that much. In many other classifiers, there are a number of hyper parameters to tune, so let us see an example of how performance is improved using Grid Search. We take an example of **Artificial Neural Networks.**\n",
        "\n",
        "You need not understand the working behind ANN, so it is okay if you do not understand the parameter grid in detail. Let's just see how the performance improves by applying Grid Search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K28nx24bNTMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use diabetes data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2W1Ei3Ugh7y",
        "colab_type": "text"
      },
      "source": [
        "**Step 1:** Define a parameter Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX_8tDiSgh7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameter_space = {\n",
        "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'alpha': [0.0001, 0.05],\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "}"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kcl_8hc6gh79",
        "colab_type": "text"
      },
      "source": [
        "**Step 2:** Fit the model to find the best hyperparameters on training data, and select the scorer you want to select to optimise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc9VN84Ygh7-",
        "colab_type": "text"
      },
      "source": [
        "<blockquote> <i>  This code takes a long time to run, you can either skip running this part and directly just see the printed results, or wait for 10-15 mins for this to run </blockquote>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBeiT6D1gh7_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "09efbc4b-269e-4cbe-e44a-89117c9e163a"
      },
      "source": [
        "mlp_random = GridSearchCV(mlp, parameter_space, scoring = 'accuracy')\n",
        "mlp_random.fit(x_train, y_train)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=None, error_score=nan,\n",
              "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
              "                                     batch_size='auto', beta_1=0.9,\n",
              "                                     beta_2=0.999, early_stopping=False,\n",
              "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
              "                                     learning_rate='constant',\n",
              "                                     learning_rate_init=0.001, max_fun=15000,\n",
              "                                     max_iter=1000, momentum=0.9,\n",
              "                                     n_iter_no_change=10,\n",
              "                                     nesterovs_momentum=True, power_t=0.5,\n",
              "                                     random_s...\n",
              "                                     validation_fraction=0.1, verbose=False,\n",
              "                                     warm_start=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'activation': ['tanh', 'relu'],\n",
              "                         'alpha': [0.0001, 0.05],\n",
              "                         'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50),\n",
              "                                                (100,)],\n",
              "                         'learning_rate': ['constant', 'adaptive'],\n",
              "                         'solver': ['sgd', 'adam']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='accuracy', verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8f-duE9gh8J",
        "colab_type": "text"
      },
      "source": [
        "**Step 3:** Print the best obtained parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4bj3qClgh8K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3e1cbc01-6daf-4ca7-e301-a8b87563dc09"
      },
      "source": [
        "mlp_random.best_params_"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'relu',\n",
              " 'alpha': 0.05,\n",
              " 'hidden_layer_sizes': (100,),\n",
              " 'learning_rate': 'constant',\n",
              " 'solver': 'adam'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmHmS7Nfgh8S",
        "colab_type": "text"
      },
      "source": [
        "**Step 4:** Train your model on these parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2repqpLogh8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp_grid = MLPClassifier(solver='adam', learning_rate='constant', hidden_layer_sizes=(100,), alpha=0.0001, \n",
        "                         activation='tanh',max_iter=2000)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCXtKB2Ugh8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp_grid.fit(x_train, y_train)\n",
        "y_pred = mlp_grid.predict(x_test)\n",
        "acc_tuned = accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZTgmXQogh8e",
        "colab_type": "text"
      },
      "source": [
        "**Comparing with Accuracy from model without hyperparameter tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "H-kojEW-gh8f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c11c2b2c-2248-4dca-a68e-5ccd21ebd515"
      },
      "source": [
        "print(\"Accuracy of Tuned model: \",np.round(acc_tuned,3))\n",
        "print(\"Accuracy of non-Tuned model: \",np.round(acc,3))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Tuned model:  0.688\n",
            "Accuracy of non-Tuned model:  0.688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsGwxzNOgh8h",
        "colab_type": "text"
      },
      "source": [
        "Approximately 5% difference in accuracy!  \n",
        "By including an even more exhaustive grid search, we can improve the performance even further"
      ]
    }
  ]
}